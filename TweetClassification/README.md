# Artifical Intellegnce
Repo for CS5100

##Assignment 4

### Problem 1

#####RUN / READ:

Classifier.py runs from the command line given a text file. It then
reads the lines of this file focusing only on the tweet and the handle. 
It returns a list of handle / tweet pairs.

#####CLEANUP:

The cleanup function ingests this output and removes stop words
and words of less than 3 character as it is unlikely that 3 character
words will have significant meaning to the overall corpus.
This is also where 30% of the ingested tweets are sectioned off for testing
and are written out to a separate file.

#####TRAIN:

The train function takes in the cleaned up list of handle/tweet pairs
and uses the Counter classes to computer the class priors, feature priors, and likelihoods.
These are used in the predict method in this file. These items are then pass the prob_dict, which
computes the probability of each element in the dictionary and returns it. These
probability dictionaries hold the requested probabilities. The first part of the tuple is
a dictionary of class (handle) probabilities, the second part is a dictionary of each word
to its probability, and the third part is a dictionary of each class to a dictionary 
of a word to its given probability in that class.

The second tuple's elements are a list of list containing a single tweet as a single string and 
the appropriate handle to go with it. This is used for the sciki portion of the problem as
the text classification demos on the scikit site show the CountVectorizer ingests the documents in this fashion
and MultinomialNB ingests the sparse matrix generated by the CountVectorizer.

I choose to do this way to pit my dictionary/Counter style of naive bayes vs naive bayes
of scikit-learn. In order to fulfill the requirements of the 
assignment of having to calculate the probabilities in train as well as produce output
usable by the scikit-learn module. Otherwise, you could simply
take the string joining in the train function ang push that to the cleanup function
and that would give you the necessary input for the scikit-learn module without
requiring the use of np.array. This is all prefaced on not wanting to 
manually generate and use a sparse matrix when a dictionary is
essentially the same thing. This also leads to a more interesting comparision.

#####PREDICT:

This takes in a tweet (string) and the elements first tuple returned by train
returns the list of classes it might belong to with decreasing probability.
This is run when evaluation is called to test the classifier.


#####EVALUATION:

This takes in two list of handles, true classification and 
predicted classification, and computes the precision, recall, and
f1 score per class. This runs after training and reads in the test
tweets and preforms the evaluation on the results.

Problem 2

Output from my implementation of Naive Bayes:
```
Reading file tweets.csv

Read 6444 tweets

Separating Tweets for Testing, Cleaning Tweets

Sectioned 1921 (29%) examples for testing

Computing Class priors, Feature Priors, and Feature/Class Likelihood

Training Complete

Beginning Testing

Reading file test_file.txt

Read 1921 tweets

Starting Evaluation

|                 	| precision      	| recall         	| f1-score       	| support 	|
|-----------------	|----------------	|----------------	|----------------	|---------	|
| HillaryClinton  	| 0.826132771338 	| 0.831389183457 	| 0.828752642706 	| 963     	|
| realDonaldTrump 	| 0.836419753086 	| 0.831288343558 	| 0.833846153846 	| 958     	|
| avg / total     	| 0.831276262212 	| 0.831288343558 	| 0.831299398276 	| 1921    	|
```
Output from SciKit Naive Bayes
````
Reading file tweets.csv

Read 6444 tweets

Separating Tweets for Testing, Cleaning Tweets

Sectioned 1933 (29%) examples for testing

Computing Class priors, Feature Priors, and Feature/Class Likelihood

Training Complete

Beginning Testing

Reading file test_file.txt

Read 1933 tweets

|                 	| precision 	| recall 	| f1-score 	| support 	|
|-----------------	|-----------	|--------	|----------	|---------	|
| HillaryClinton  	| 0.90      	| 0.92   	| 0.91     	| 972     	|
| realDonaldTrump 	| 0.91      	| 0.90   	| 0.91     	| 961     	|
| avg / total     	| 0.91      	| 0.91   	| 0.91     	| 1933    	|
    
````
The scikit implementation is about 10% better than my implementation. This is probably
do to the following factors: I use Multinomial NB in the scikit
implementation vs vanilla NB, my implementation of a smoothing factor may have tipped
the scale in some cases. Also my choice of excluding words less than
4 characters and my static list of stopwords were probably the largest
factors accounting for this difference.

I also ran some tests with the scikit with TF-IDF and it performed worse
though this is likely due to TF-IDF being implemented on an already
cleaned up corpus. When the filtering was removed, perfomance was slightly improved.